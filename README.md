# PySpark and Apache Kafka projects, with data in Batch and Streaming

This repository contains a series of Infrastructure as Code files that I made to allocate resources in the Cloud: AWS, Azure and GCP.

## Overview

This repository contains a series of Data Processing Projects, using PySpark and Kafka with various Batch and Stream Data different setting. 

Each project has its particularity in the architecture and tool used. There are also cases where the volume is changed.

The environment used is Docker at all times, with no Cloud being used. I created the cluster orchestration and scaling scripts, within Docker.


<br/><br/>

## PySpark and Apache Kafka

 <img width="2500px" align="right"  src="https://github.com/julianasantimaria/ProjectsTerraform/blob/HTML/IaC.jpeg">

 <br/>
 <br/>


## Project Structure

The structuring of this repository is difficult and does not require Frontend. It is a series of Projects with various scenarios of structuring and moving Data, using container scalability, all in Docker and scenarios of Data loads in Batch and Streaming.

## Requisites

Make sure you have Docker tools and install PySpark and Kafka.

```bash
- Docker
- Apache Kafka
- PySpark